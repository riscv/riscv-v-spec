= RISC-V "V" Vector Extension
Version 0.7-draft-20181231
:doctype: article
:encoding: utf-8
:lang: en
:toc: left
:numbered:
:stem: latexmath

== Introduction

This document describes the draft of the RISC-V vector extension.  The
document describes all the individual features of the base vector
extension. Some platforms may choose to implement subsets of these
extensions as described below.

NOTE: This is a draft of a stable proposal for the vector
specification to be used for implementation and evaluation.  This
version is intended to be stable enough to begin developing detailed
encoding, toolchains, functional simulators, and initial
implementations.

== Implementation-defined constant parameters

Each hart supporting the vector extension defines two parameters.

The maximum size of a single vector element in bits, _ELEN_.
Must be a power of 2.

The number of bits in a vector register, _VLEN_ >= _ELEN_.  Must
be a power of 2.

NOTE: Platform profiles may set further constraints on these
parameters, for example, requiring that ELEN >=
max(XLEN,FLEN) or requiring a minimum VLEN value.

The ISA supports writing binary code that under certain constraints
will execute portably on harts with different values for these
parameters.

NOTE: Code can be written that will expose differences in
implementation parameters.

NOTE: Thread contexts with active vector state cannot be migrated
during execution between harts that have different VLEN and ELEN
parameters.

== Vector Extension Programmer's Model

The vector extension adds 32 vector registers, and three unprivileged
XLEN-width CSRs (`vtype`, `vl`, and `vlmax`) to the base scalar RISC-V ISA.

=== Vector Registers

The vector extension adds 32 architectural vector registers,
`v0`-`v31` to the base scalar RISC-V ISA.

Each vector register has a fixed VLEN bits of state.

NOTE: To increase readability, vector register layouts are drawn with
bytes ordered from right to left with increasing byte address.  Bits
within an element are numbered in a little-endian format with
increasing bit index from right to left corresponding to increasing
magnitude.

NOTE: Zfinx ("F in X") is a new ISA option under consideration where
floating-point instructions take their arguments from the integer
register file.  The 0.7 vector extension is also compatible with this
option.

=== Vector type register, `vtype`

The XLEN-wide _vector_ _type_ CSR, `vtype` provides the default type
used to interpret the contents of the vector register file. The vector
type also determines the organization of elements in each vector
register, and how multiple vector registers are grouped.

The `type` register has three fields, `vrep`, `vsew[2:0]`, and
`vlmul`.

[source]
----
vtype layout

XLEN-1:9    Reserved (write 0)
       8    vrep
     7:6    vlmul[1:0]
     5:0    vsew[5:0]
----

NOTE: Further standard and custom extensions to the vector base will
extend these three fields to support a greater variety of data types.

==== Vector representation `vrep` encoding

The `vrep` field specifies how the bit patterns stored in each element
are to be interpeted by default.  Instructions may explicitly override
the default representation.

[source]
----
 'vrep' representation field encoding

 0  Signed two's-complement integer
 1  IEEE-754/2008 floating-point
----

==== Vector standard element width `vsew`

The value in `vsew` sets the dynamic _standard_ _element_ _width_
(SEW).  By default, a vector register is viewed as being divided
into VLEN / SEW standard-width elements.

[source]
----
  vsew[5:0] (standard element width) encoding

  vsew[5:0]  SEW
  ---        ----
  000000        8
  000001       16
  000010       32
  000011       64
  000100      128
  000101      256
  000110      512
  000111     1024
  xxx???     reserved xxx!=0 
----

[source]
----
  Example VLEN = 128 bits

  SEW  Elements per vector register
   64     2
   32     4
   16     8
    8    16
----

==== Vector register grouping and `vlmul` field

To provide greater efficiency on longer vectors, multiple vector
registers can be grouped together to form a _vector_ _register_
_group_, so that a single vector instruction can operate on multiple
vector registers.  Vector register grouping also allows double-width
or larger elements to be operated on with the same vector length as
standard-width elements.

The number of vector registers in a group, _LMUL_, is an integer power
of two set by the `vlmul` field in `vtype`.  The maximum vector length
possible in a single vector instruction, VLMAX, is then increased by
a factor of LMUL.

[source]
----
 vlmul  LMUL  #groups   VLMAX        Grouped registers
 00       1       32    VLEN/SEW     none
 01       2       16    2*VLEN/SEW   vn, vn+1
 10       4        8    4*VLEN/SEW   vn, ..., vn+3
 11       8        4    8*VLEN/SEW   vn, ..., vn+7
----

When `vlmul=01`, then vector operations on register ``v``__n__ also
operate on vector register``v``__n__+1, giving twice the vector length
in bits.  Instructions specifying a vector operand with an
odd-numbered vector register will raise an illegal instruction
exception.

Similarly, when `vlmul=10`, every vector operation operates on four
vector registers at a time, and instructions specifying vector
operands using vector register numbers that are not multiples of four
will raise an illegal instruction exception.  When `vlmul=11`,
operations operate on eight vector registers at a time, and
instructions specifying vector operands using register numbers that
are not multiples of eight will raise an illegal instruction
exception.

NOTE: Scalar operands, described below, can come from any vector
register regardless of the `vlmul` setting.

=== Vector Length register `vl`

The _XLEN_-bit-wide read-only `vl` CSR can only be updated by the
`vsetvli` and `vsetvl` instructions.

The `vl` register holds an unsigned integer specifying the number of
elements to be updated by a vector instruction.  Elements in the
destination vector with indices >= `vl` are not updated during
execution of a vector instruction.  As a degenerate case, when `vl`=0,
no elements are updated in the destination vector.

=== Maximum vector Length register `vlmax`

The XLEN-wide `vlmax` CSR is a read-only register whose value is
derived from the other state in the system.  The `vlmax` register
holds an unsigned integer representing the largest number of elements
that can be completed by a single vector instruction with the current
`vtype` setting.  The value in `vlmax` = LMUL * VLEN / SEW.

== Mapping of vector elements to vector register state

The following diagrams illustrate how different width elements are
packed into the bytes of a vector register depending on the current
SEW and LMUL settings, as well as implementation ELEN and VLEN.
Elements are packed into each vector register with the
least-significant byte in the lowest-numbered bits.

NOTE: Previous RISC-V vector proposals (< 0.6) hid this mapping from
software, whereas this proposal has a specific mapping for all
configurations, which reduces implementation flexibilty but removes
need for zeroing on config changes.  Making the mapping explicit also
has the advantage of simplifying oblivious context save-restore code,
as the code can save the configuration in `vl`, `vlmax`, and `vtype`,
then reset `vtype` to a convenient value (e.g., four vector groups of
LMUL=8, SEW=ELEN) before saving all vector register bits without
needing to parse the configuration.  The reverse process will restore
the state.

=== Mapping with LMUL=1

When LMUL=1, elements are simply packed in order from the
least-significant to most-significant bits of the vector register.

[source]
----
  The element index is given in hexadecimal and is shown placed at the least-significant byte of the stored element. ELEN <=128 and LMUL=1 throughout.


 VLEN=32b

 Byte         3 2 1 0

 SEW=8b       3 2 1 0
 SEW=16b        1   0
 SEW=32b            0

 VLEN=64b

 Byte        7 6 5 4 3 2 1 0

 SEW=8b      7 6 5 4 3 2 1 0
 SEW=16b       3   2   1   0
 SEW=32b           1       0
 SEW=64b                   0

 VLEN=128b

 Byte        F E D C B A 9 8 7 6 5 4 3 2 1 0

 SEW=8b      F E D C B A 9 8 7 6 5 4 3 2 1 0
 SEW=16b       7   6   5   4   3   2   1   0
 SEW=32b           3       2       1       0
 SEW=64b                   1               0
 SEW=128b                                  0

 VLEN=256b

 Byte     1F1E1D1C1B1A19181716151413121110 F E D C B A 9 8 7 6 5 4 3 2 1 0

 SEW=8b   1F1E1D1C1B1A19181716151413121110 F E D C B A 9 8 7 6 5 4 3 2 1 0
 SEW=16b     F   E   D   C   B   A   9   8   7   6   5   4   3   2   1   0
 SEW=32b         7       6       5       4       3       2       1       0
 SEW=64b                 3               2               1               0
 SEW=128b                                1                               0
----

=== Mapping with LMUL > 1

When vector registers are grouped, the elements of the vector register
group are striped across the constituent vector registers.

When VLEN > 128 and SEW =< 128, the striping pattern is repeated in
multiples of 128 bits.  The first 128/SEW elements are packed
contiguously at the start of the first vector register in the group.
The next 128/SEW elements are packed contiguously at the start of the
next vector register in the group.  After packing the first
LMUL*128/SEW elements at the start of each of the LMUL vector
registers in the group, the second LMUL*128/SEW group of elements are
packed into the second 128b segment of each of the vector registers in
the group, and so on.

[source]
----
 Example 1: VLEN=32b, SEW=16b, LMUL=2

 Byte         3 2 1 0
 v2*n           1   0
 v2*n+1         3   2

 Example 2: VLEN=64b, SEW=32b, LMUL=2

 Byte         7 6 5 4 3 2 1 0
 v2*n               1       0
 v2*n+1             3       2

 Example 3: VLEN=128b, SEW=32b, LMUL=2

 Byte        F E D C B A 9 8 7 6 5 4 3 2 1 0
 v2*n              3       2       1       0
 v2*n+1            7       6       5       4

 Example 4: VLEN=256b, SEW=32b, LMUL=2

 Byte     1F1E1D1C1B1A19181716151413121110 F E D C B A 9 8 7 6 5 4 3 2 1 0
 v2*n            B       A       9       8       3       2       1       0
 v2*n+1          F       E       D       C       7       6       5       4
----

When SEW > 128, the striping pattern places one element in each vector
register in the group before moving to the next vector register in the
group.  So, when LMUL=2, the even-numbered vector register contains
the even-numbered elements of the vector and the odd-numbered vector
register contains the odd-numbered elements of the vector.

[source]
----
 Example: VLEN=256b, SEW=256b, LMUL=2
 
 Byte     1F1E1D1C1B1A19181716151413121110 F E D C B A 9 8 7 6 5 4 3 2 1 0
 v2*n                                                                    0
 v2*n+1                                                                  1

----

When LMUL = 4, four vector registers hold elements as shown:

[source]
----
 Example 1: VLEN=32b, SEW=16b, LMUL=4

 Byte         3 2 1 0
 v4*n           1   0
 v4*n+1         3   2
 v4*n+2         5   4
 v4*n+3         7   6

 Example 2: VLEN=64b, SEW=32b, LMUL=4

 Byte         7 6 5 4 3 2 1 0
 v4*n               1       0
 v4*n+1             3       2
 v4*n+2             5       4
 v4*n+3             7       6


 Example 3: VLEN=128b, SEW=32b, LMUL=4

 Byte          F E D C B A 9 8 7 6 5 4 3 2 1 0
 v4*n                3       2       1       0   32b elements
 v4*n+1              7       6       5       4
 v4*n+2              B       A       9       8
 v4*n+3              F       E       D       C

 Example 4: VLEN=256b, SEW=32b, LMUL=4

 Byte     1F1E1D1C1B1A19181716151413121110 F E D C B A 9 8 7 6 5 4 3 2 1 0
 v4*n           13      12      11      10       3       2       1       0
 v4*n+1         17      16      15      14       7       6       5       4
 v4*n+2         1B      1A      19      18       B       A       9       8
 v4*n+3         1F      1E      1D      1C       F       E       D       C

 Example 5: VLEN=256b, SEW=256b, LMUL=4
 
 Byte     1F1E1D1C1B1A19181716151413121110 F E D C B A 9 8 7 6 5 4 3 2 1 0
 v4*n                                                                    0
 v4*n+1                                                                  1
 v4*n+2                                                                  2
 v4*n+3                                                                  3
----

A similar pattern is followed for LMUL = 8.

[source]
----
 Example: VLEN=256b, SEW=32b, LMUL=8

 Byte   1F1E1D1C1B1A19181716151413121110 F E D C B A 9 8 7 6 5 4 3 2 1 0
 v8*n         23      22      21      20       3       2       1       0
 v8*n+1       27      26      25      24       7       6       5       4
 v8*n+2       2B      2A      29      28       B       A       9       8
 v8*n+3       2F      2E      2D      2C       F       E       D       C
 v8*n+4       33      32      31      30      13      12      11      10
 v8*n+5       37      36      35      34      17      16      15      14
 v8*n+6       3B      3A      39      38      1B      1A      19      18
 v8*n+7       3F      3E      3D      3C      1F      1E      1D      1C
----

NOTE: The striping pattern is a compromise between the datapath wiring
needed for mixed-width operations and the buffering needed to
corner-turn wide vector unit-stride memory accesses into parallel
accesses for the vector register file.  The value 128b also matches
the widest standard integer (RV128) and floating-point types (Q).  The
previous 0.6 design optimized datapath wiring but required buffering
up to 8 element groups when accessing the vector regfile (or
alternatively using multiple narrower ports) regardless of width of
memory port.  The current 0.7 design requires mixed-width operands to
cross no more than 128b of datapath, while spatial locality of vector
regfile storage access is increased to 128b per write.  When the
unit-stride memory access width is greater than 128b, values will
still need to be buffered or wider regfile ports used.

NOTE: The previous explicit configuration design allowed these
tradeoffs to be managed at the microarchitectural level and optimized
for each configuration.

NOTE: The striping width (SLEN=128b) could be an
implementation-dependent parameter, for example, based on min(VLEN,
max(ELEN,128)).

=== Mapping across mixed-width operations

The pattern used to map elements within a vector register group is
designed to reduce datapath wiring when supporting operations across
multiple element widths.  The recommended pattern in this case is to
modify `vtype` dynamically to keep SEW/LMUL constant (and hence VLMAX
constant).

The following example shows four different packed element widths (8b,
16b, 32b, 64b) in a VLEN=256b implementation.  The vector register
grouping factor (LMUL) is increased by the relative element size such
that each group can hold the same number of vector elements (32 in
this example) to simplify stripmining code.  Any operation between
elements with the same index only touches operand bits located within
the same 128b portion of the datapath.

[source]
----
 VLEN=256b
 Byte     1F1E1D1C1B1A19181716151413121110 F E D C B A 9 8 7 6 5 4 3 2 1 0

 SEW=8b, LMUL=1, VLMAX=32

 v1       1F1E1D1C1B1A19181716151413121110 F E D C B A 9 8 7 6 5 4 3 2 1 0

 SEW=16b, LMUL=2, VLMAX=32

 v2*n       17  16  15  14  13  12  11  10   7   6   5   4   3   2   1   0
 v2*n+1     1F  1E  1D  1C  1B  1A  19  18   F   E   D   C   B   A   9   8

 SEW=32b, LMUL=4, VLMAX=32

 v4*n           13      12      11      10       3       2       1       0
 v4*n+1         17      16      15      14       7       6       5       4
 v4*n+2         1B      1A      19      18       B       A       9       8
 v4*n+3         1F      1E      1D      1C       F       E       D       C

 SEW=64b, LMUL=8, VLMAX=32

 v8*n                   11              10               1               0
 v8*n+1                 13              12               3               2
 v8*n+2                 15              14               5               4
 v8*n+3                 17              16               7               6
 v8*n+4                 19              18               9               8
 v8*n+5                 1B              1A               B               A
 v8*n+6                 1D              1C               D               C
 v8*n+7                 1F              1E               F               E
----

Larger LMUL settings can also used to simply increase vector length to
reduce instruction fetch and dispatch overheads, in cases where fewer
logical vector registers are required.

Larger LMUL values can cause lower datapath utilization for short
vectors.  The `setvl` instructions below have a facility to
dynamically select an appropriate LMUL according to the required
application vector length (AVL).

=== Mask register layout

A vector mask occupies only one vector register regardless of SEW and LMUL.
The mask bits that are used for each vector operation depends on the
current SEW and LMUL setting.

[source]
----
The maximum number of elements in a vector operand is:

               VLMAX = LMUL * VLEN/SEW

A mask is allocated for each element by dividing the mask register into
VLEN/VLMAX fields.  The size of each mask field in bits (MASKBITS) is:

               MASKBITS = VLEN/VLMAX
                        = VLEN/(LMUL * VLEN/SEW)
                        = SEW/LMUL
----

The pattern is such that for constant SEW/LMUL values, the mask bits
are located in the same bit of the mask vector register, which
simplifies use of masking in loops with mixed-width elements.

The size of MASKBITS varies from ELEN (SEW=ELEN, LMUL=1) down to 1
(SEW=8b,LMUL=8), and hence a single vector register can always hold
the entire mask register.

The mask bits for element _i_ are located in bits
[MASKBITS*_i_+(MASKBITS-1),MASKBITS*_i_] of the mask register.  When a
mask value is written by a compare instruction, the low bit in the
mask field holds the mask result and the upper bits of the mask field
are zeroed.  When a value is read as a mask, only the
least-significant bit of the mask field is used to control masking and
the upper bits are ignored.



[source]
----
 VLEN=32b

          Byte    3   2   1   0
 LMUL=1,SEW=8b
                  3   2   1   0  Element
                [24][16][08][00] Mask bit position

 LMUL=2,SEW=16b
                      1       0
                    [08]    [00]
                      3       2
                    [24]    [16]

 LMUL=4,SEW=32b               0
                            [00]
                              1
                            [08]
                              2
                            [16]
                              3
                            [24]
----

[source]
----
 LMUL=2,SEW=8b
                  3   2   1   0
                [12][08][04][00]
                  7   6   5   4
                [28][24][20][16]

 LMUL=8,SEW=32b
                              0
                            [00]
                              1
                            [04]
                              2
                            [08]
                              3
                            [12]
                              4
                            [16]
                              5
                            [20]
                              6
                            [24]
                              7
                            [28]

 LMUL=8,SEW=8b
                  3   2   1   0
                [03][02][01][00]
                  7   6   5   4
                [07][06][05][04]
                  B   A   9   8
                [11][10][09][08]
                  F   E   D   C
                [15][14][13][12]
                 13  12  11  10
                [19][18][17][16]
                 17  16  15  14
                [23][22][21][20]
                 1B  1A  19  18
                [27][26][25][24]
                 1F  1E  1D  1C
                [31][30][29][28]
----

[source]
----
 VLEN=256b
 Byte     1F1E1D1C1B1A19181716151413121110 F E D C B A 9 8 7 6 5 4 3 2 1 0

 SEW=8b, LMUL=1, VLMAX=32

 v1       1F1E1D1C1B1A19181716151413121110 F E D C B A 9 8 7 6 5 4 3 2 1 0

 SEW=16b, LMUL=2, VLMAX=32

 v2*n       17  16  15  14  13  12  11  10   7   6   5   4   3   2   1   0
 v2*n+1     1F  1E  1D  1C  1B  1A  19  18   F   E   D   C   B   A   9   8

 SEW=32b, LMUL=4, VLMAX=32

 v4*n           13      12      11      10       3       2       1       0
 v4*n+1         17      16      15      14       7       6       5       4
 v4*n+2         1B      1A      19      18       B       A       9       8
 v4*n+3         1F      1E      1D      1C       F       E       D       C

 SEW=64b, LMUL=8, VLMAX=32

 v8*n                   11              10               1               0
 v8*n+1                 13              12               3               2
 v8*n+2                 15              14               5               4
 v8*n+3                 17              16               7               6
 v8*n+4                 19              18               9               8
 v8*n+5                 1B              1A               B               A
 v8*n+6                 1D              1C               D               C
 v8*n+7                 1F              1E               F               E
----

== Configuration-setting instructions

A set of instructions are provided to allow rapid configuration of the
values in `vl` and `vtype` to match application needs.

=== `vsetvli`/`vsetvl` instructions

----
 vsetvli rd, rs1, vtypei # rd = new vl, rs1 = AVL, vtypei = new vtype setting
                         # if rs1 = x0, then use maximum vector length
 vsetvl  rd, rs1, rs2    # rd = new vl, rs1 = AVL, rs2 = new vtype value
                         # if rs1 = x0, then use maximum vector length
----

The `vsetvli` instruction sets the `vtype`, `vl`, and `vlmax` CSRs
based on its arguments, and writes the new value of `vl` into `rd`.

The new `vtype` setting is encoded in the immediate field `vtypei` for
`vsetvli` and in the `rs2` register for `vsetvl`.

[source]
----
 Suggested assembler names used for vtypei setting

 vint8    #   8b signed integers
 vint16   #  16b signed integers
 vint32   #  32b signed integers
 vint64   #  64b signed integers
 vint128  # 128b signed integers

 vfp16    #  16b IEEE FP
 vfp32    #  32b IEEE FP
 vfp64    #  64b IEEE FP
 vfp128   # 128b IEEE FP

 vlmul1   # Vlmul x1
 vlmul2   # Vlmul x2
 vlmul4   # Vlmul x4
 vlmul8   # Vlmul x8

 vlmul1max   # Vlmul x1 max
 vlmul2max   # Vlmul x2 max
 vlmul4max   # Vlmul x4 max
 vlmul8max   # Vlmul x8 max
----

NOTE: The immediate argument `vtypei` can be a compressed form of the
full vtype setting, capturing the most common use cases.  For the base
proposed here, it is assumed that at least four bits of immediate are
available to write all standard values of `vtype` (`vsew[2:0]`,
`vlmul`, and `vrep`).

The `vtype` setting must be supported by the implementation, and the
 `vsetvl{i}` instructions will raise an illegal instruction exception
 if the setting is not supported.

NOTE: Specifing that `vtype` is WARL is problematic as that would hide
errors.  The current spec is problematic in that it requires a trap
based on a data value in a CSR write. It would simplify pipelines if
`vtype` value errors were flagged at use not write, but somehow need
to catch errant code without requiring full XLEN bits in `vtype` when
only a few bits are actually used. One alternative is to allow
substitution of a fixed illegal value in `vtype`, e.g., all 1s, if an
attempt is made to write an unsupported value.  This would then cause
a trap on use.

The requested application vector length (AVL) is passed in `rs1` as an
unsigned integer.

The `vlmax` register is set to VLMAX based on the new SEW and LMUL in
the `vtype` setting.

=== Constraints on setting `vl`

The resulting `vl` setting must satisfy the following constraints:

. `vl = AVL` if `AVL \<= VLMAX`
. `vl >= ceil(AVL / 2)` if `AVL < (2 * VLMAX)`
. `vl = VLMAX` if `AVL >= (2 * VLMAX)`
. Deterministic on any given implementation for same input AVL and `vtype` values
. These specific properties follow from the prior rules:
.. `vl = 0` if  `AVL = 0`
.. `vl > 0` if `AVL > 0`
.. `vl \<= VLMAX`
.. `vl \<= AVL`


[NOTE]
--
The `vl` setting rules are designed to be sufficiently strict to
preserve `vl` behavior across register spills and context swaps for
`AVL \<= VLMAX`, yet flexible enough to enable implementations to improve
vector lane utilization for `AVL > VLMAX`.

For example, this permits an implementation to set `vl = ceil(AVL / 2)`
for `VLMAX < AVL < 2*VLMAX` in order to evenly distribute work over the
last two iterations of a stripmine loop.
Requirement 2 ensures that the first stripmine iteration of reduction
loops uses the largest vector length of all iterations, even in the case
of `AVL < 2*VLMAX`.
This allows software to avoid needing to explicitly calculate a running
maximum of vector lengths observed during a stripmined loop.
--

=== Rules for vlmul

Shorter AVL can give low datapath utilization for longer LMUL.  The
`vlmul` parameter can optionally be supplied as a maximum value, in
which case the following rules are used to set `vlmul`.

[source]
----
Rule for setting LMUL based on vlmulmax value

VLMAX1 = VLEN/SEW     # Elements fitting in one vector register

       0 < AVL <= VLMAX1,   LMUL = 1                # Don't group
VLMAX1   < AVL <= VLMAX1*2, LMUL = min(2,vlmulmax)  # Pairs
VLMAX1*2 < AVL <= VLMAX1*4, LMUL = min(4,vlmulmax)  # Quads
VLMAX1*4 < AVL              LMUL = min(8,vlmulmax)  # Octa

----

=== `vsetvl` instruction

The `vsetvl` variant operates similary to `vsetvli` except that it
takes a `vtype` value from `rs2` and can be used for context restore,
and when the `vtypei` field is too small to hold the desired setting.

NOTE: Several active complex types can be held in different `x`
registers and swapped in as needed using `vsetvl`.

=== Examples

The SEW and LMUL settings can be changed dynamically to provide high
throughput on mixed-width operations in a single loop.

[source]
----
# Example: Load 16-bit values, widen multiply to 32b, shift 32b result
# right by 3, store 32b values.

# Loop using only widest elements:

loop:
    vsetvli a3, a0, vint32,vlmul8max   # Use only 32-bit elements
    vlh.v v8, (a1)          # Sign-extend 16b load values to 32b elements
      sll t1, a3, 1
      add a1, a1, t1        # Bump pointer
    vmul.vs  v8, v8, v1     # 32b multiply result
    vsrl.vi  v8, v8, 3      # Shift elements
    vsw.v v8, (a2)          # Store vector of 32b results
      sll t1, a3, 2
      add a2, a2, t1        # Bump pointer
      sub a0, a0, a3        # Decrement count
      bnez a0, loop         # Any more?

# Alternative loop that switches element widths.

loop:
    vsetvli a3, a0, vint16,vlmul4max   # vtype = 16-bit integer vectors
    vlh.v v4, (a1)          # Get 16b vector
      slli t1, a3, 1
      add a1, a1, t1        # Bump pointer
    vmulw.vs v8, v4, v1     # 32b in <v8--v15>

    vsetvli x0, a0, vint32,vlmul8max   # Operate on 32b values
    vsrl.vi v8, v8, 3
    vsw.v v8, (a2)          # Store vector of 32b
      slli t1, t1, 2
      add a2, a2, t1        # Bump pointer
      sub a0, a0, a3        # Decrement count
      bnez a0, loop         # Any more?

The second loop is more complex but will have greater performance on
machines where 16b widening multiplies are faster than 32b integer
multiplies, and where 16b vector load can run faster due to the
narrower writes to the vector regfile.

The vlmul?max versions are used here to reduce LMUL for shorter vector
lengths.  The same AVL must be supplied to both the 16-bit/LMULMAX=4
and the 32-bit/LMULMAX=8 vsetvli instructions.
----

== Vector masking

Masking is supported on almost all vector instructions producing
vectors.  Element operations that are masked off do not modify the
destination vector register element and never generate exceptions.
Instructions producing scalars are not maskable.

In the base vector extension, the mask value is always supplied by
vector register `v0`.

NOTE: Future vector extensions may provide longer instruction
encodings with space for a full mask register specifier.

Masking is encoded in a two-bit `m[1:0]` field (`inst[26:25]`) for all
vector instructions.

[source]
----
m[1:0]

  00    vector, where v0[0] = 0
  01    vector, where v0[0] = 1
  10    scalar operation
  11    vector, always true
----

=== Active Element Definition

The _active_ elements in a vector instruction are the destination
register elements for _x_, where 0 =< _x_ < `vl`, and where the element
position is active under the current mask.  By default, these are the
only elements updated in the destination vector register.

NOTE: Earlier versions of the spec wrote 0 to the inactive elements of
a vector register to simplify implementations with vector register
renaming or that use ECC in vector register storage.  Leaving inactive
elements in destination unmodified reduces instruction count in more
situations than zeroing elements, and simplifies implementations
without register renaming.

[source]
----

# Code using one width for predicate and different width for masked
# compute.
#   int8_t a[]; int32_t b[], c[];
#   for (i=0;  i<n; i++) { b[i] =  (a[i] < 5) ? 5 : c[i]; }
#
  loop:
    vsetvli a4, a0, vint8,vlmul1  # Byte vector for predicate
    vlb.v v0, (a1)                # Load a[i]
      add a1, a1, a4              # Bump pointer.
      sub a0, a0, a4              # Decrement count
    vslti.v v0, v0, 5             # a[i] < 5?

    vsetvtypei vint32,vlmul4      # Vector of 32 bit values.
    vmerge.vi v4, v4, 5           # Initialize v4.
    vlw.v v4, (a3), v0.f          # Overwrite with c[i] under mask.
      sll t1, a4, 2
      add a3, a3, t1              # Bump pointer.
    vsw.v v4, (a2)                # Store b[i].
      add a2, a2, t1              # Bump pointer.
      bnez a0, loop               # Any more?
----

== Vector floating-point exception flags

A vector floating-point exception at any active floating-point element
sets the standard FP exception flags in the `fcsr` register.  Masked
elements do not set FP exception flags.

== Vector instruction formats

Vector loads and stores move bit patterns between vector register
elements and memory.  Vector arithmetic instructions operate on values
held in vector register elements.

Vector instructions can have scalar or vector source operands and
produce scalar or vector results.  Vector operands or results may
occupy one or more vector registers depending on LMUL, but are always
specified using the lowest-numbered vector register in the group.
Using other than the lowest-numbered vector register to specify a
vector register group will result in an illegal instruction exception.

Scalar operands and results are located in element 0 of a vector
register.  A scalar operand can be in any vector register regardless
of the current LMUL setting.

== Vector Loads and Stores

Vector loads and stores are encoding within the scalar floating-point
load and store major opcodes (LOAD-FP/STORE-FP).

=== Operation of Floating-Point Load/Store Instructions in Vector Extension

The standard FDQ floating-point extensions' loads and stores retain
their original meaning.

The standard floating-point loads (FLH, FLW, FLD, FLQ), read a single
value from memory and update the low stem:[FLEN] bits of the
destination vector register.  Floating-point types narrower than
stem:[FLEN] are NaN-boxed, setting upper bits to 1.  If stem:[VLEN >
FLEN], the upper bits of the vector register are unchanged by the
floating-point load.

The standard floating-point stores (FSH, FSW, FSD, FDQ) read the
appropriate number of bits from the least-significant bits of the
vector register and write these to memory.

=== Vector Load/Store Instruction Encoding

The vector loads and stores are encoded using the width values that
are not claimed by the standard scalar floating-point loads and
stores.

[source]
----
                     Width xv  Mem     Reg       opcode uoffset5 scale
                     [2:0]     Bits    Bits             (set by width[1:0])

Standard scalar FP    001  x    16     FLEN      FLH/FSH N/A
Standard scalar FP    010  x    32     FLEN      FLW/FSW N/A
Standard scalar FP    011  x    64     FLEN      FLD/FSD N/A
Standard scalar FP    100  x   128     FLEN      FLQ/FSQ N/A
Vector byte           000  0  vl*8     vl*SEW    VxB     1
Vector halfword       101  0  vl*16    vl*SEW    VxH     2
Vector word           110  0  vl*32    vl*SEW    VxW     4
Vector doubleword     111  0  vl*64    vl*SEW    VxD     8
Vector single-width   000  1  vl*SEW   vl*SEW    VxE     1
Vector double-width   101  1  vl*2*SEW vl*2*SEW  VxE2    2
Vector quad-width     110  1  vl*4*SEW vl*4*SEW  VxE4    4
Vector octa-width     111  1  vl*8*SEW vl*8*SEW  VxE8    8

The one-bit xv field encodes fixed or variable element width, and is located in imm12 field
Mem bits is the size of element moved in memory
Reg bits is the size of element accessed in register
uoffset5 scale is the amount by which the five-bit unsigned immediate is multiplied to obtain a byte offset
----

The vector load and store encodings repurpose a portion of the
standard load/store 12-bit immediate field to provide further vector
instruction encoding, with bits[26:25] holding the mask information.

Bits [31:27] hold a 5-bit unsigned offset that is added to the base
register during vector addressing.  The offset is scaled according to
the low two bits of the width[2:0] field (effective offset =
uoffset[4:0] * 2^width[1:0]^), such that for fixed-width elements the
offset is scaled by the element size.  For dynamic-width elements, the
offset is not affected by the `vtype` setting to avoid having a
dependency between address generation and dynamic `vtype` value.

[source]
----
31302928272625242322212019181716151413121110 9 8 7 6543210
       imm[11:0]       |   rs1   | 0 0 1|   rd    |0000111 FLH
       imm[11:0]       |   rs1   | 0 1 0|   rd    |0000111 FLW
       imm[11:0]       |   rs1   | 0 1 1|   rd    |0000111 FLD
       imm[11:0]       |   rs1   | 1 0 0|   rd    |0000111 FLQ
 imm[11:5]   |   rs2   |   rs1   | 0 0 1| imm[4:0]|0100111 FSH
 imm[11:5]   |   rs2   |   rs1   | 0 1 0| imm[4:0]|0100111 FSW
 imm[11:5]   |   rs2   |   rs1   | 0 1 1| imm[4:0]|0100111 FSD
 imm[11:5]   |   rs2   |   rs1   | 1 0 0| imm[4:0]|0100111 FSQ

mop  |off| m | rs2/vs2 |   rs1   | 0 0 0|   vd    |0000111 VL

  vs3    | m | rs2/vs2 |   rs1   | 0 0 1|mop  |off|0100111 VS




funct5 encodes:
name bits encoding

Loads

Signed/Unsigned

Unit-stride
Unit-stride, FF
Constant-stride
Scatter/gather

SEW * x, doesn't need signed/unsigned


8  s/u
16 s/u
32 s/u
64 s/u


contiguous memory move

base = rs1 + (imm * size)
for (e=0; e<vl; e++)
{
  for (r = 0; r < n_reg; r++)
  {
    if (mask(e)==true)
    {
      v(d+r)[e] = M[base]
    }                                    }
    base += size;
  }
}








vl
mem elem size 1,2,4,8, bigger
reg elem size
# registers






mop  |off| m | rs2/vs2 |   rs1   | 0 0 0|   vd    |0000111 VL

0 0 0|off| m |         |   rs1   | 0 0 0|   vd    |0000111 VLB
0 0 0|off| m |         |   rs1   | 1 0 1|   vd    |0000111 VLH
0 0 0|off| m |         |   rs1   | 1 1 0|   vd    |0000111 VLW
0 0 0|off| m |         |   rs1   | 1 1 1|   vd    |0000111 VLD

0 0 0|off| m |         |   rs1   | 0 0 0|   vd    |0000111 VLBU
0 0 0|off| m |         |   rs1   | 1 0 1|   vd    |0000111 VLHU
0 0 0|off| m |         |   rs1   | 1 1 0|   vd    |0000111 VLWU
0 0 0|off| m |         |   rs1   | 1 1 1|   vd    |0000111 VLDU

0 0 0|off| m |         |   rs1   | 0 0 0|   vd    |0000111 VLBFF
0 0 0|off| m |         |   rs1   | 1 0 1|   vd    |0000111 VLHFF
0 0 0|off| m |         |   rs1   | 1 1 0|   vd    |0000111 VLWFF
0 0 0|off| m |         |   rs1   | 1 1 1|   vd    |0000111 VLDFF

0 0 0|off| m |         |   rs1   | 0 0 0|   vd    |0000111 VLBUFF
0 0 0|off| m |         |   rs1   | 1 0 1|   vd    |0000111 VLHUFF
0 0 0|off| m |         |   rs1   | 1 1 0|   vd    |0000111 VLWUFF
0 0 0|off| m |         |   rs1   | 1 1 1|   vd    |0000111 VLDUFF

0 0 0|off| m |   rs2   |   rs1   | 0 0 0|   vd    |0000111 VLSB
0 0 0|off| m |   rs2   |   rs1   | 1 0 1|   vd    |0000111 VLSH
0 0 0|off| m |   rs2   |   rs1   | 1 1 0|   vd    |0000111 VLSW
0 0 0|off| m |   rs2   |   rs1   | 1 1 1|   vd    |0000111 VLSD

0 0 0|off| m |   rs2   |   rs1   | 0 0 0|   vd    |0000111 VLSBU
0 0 0|off| m |   rs2   |   rs1   | 1 0 1|   vd    |0000111 VLSHU
0 0 0|off| m |   rs2   |   rs1   | 1 1 0|   vd    |0000111 VLSWU
0 0 0|off| m |   rs2   |   rs1   | 1 1 1|   vd    |0000111 VLSDU

0 0 0|off| m |   vs2   |   rs1   | 0 0 0|   vd    |0000111 VLXB
0 0 0|off| m |   vs2   |   rs1   | 1 0 1|   vd    |0000111 VLXH
0 0 0|off| m |   vs2   |   rs1   | 1 1 0|   vd    |0000111 VLXW
0 0 0|off| m |   vs2   |   rs1   | 1 1 1|   vd    |0000111 VLXD

0 0 0|off| m |   vs2   |   rs1   | 0 0 0|   vd    |0000111 VLXBU
0 0 0|off| m |   vs2   |   rs1   | 1 0 1|   vd    |0000111 VLXHU
0 0 0|off| m |   vs2   |   rs1   | 1 1 0|   vd    |0000111 VLXWU
0 0 0|off| m |   vs2   |   rs1   | 1 1 1|   vd    |0000111 VLXDU


mop [2:0]
   0 0 0
   0 0 1
   0 1 0
   0 1 1
   1 0 0
   1 0 1
   1 1 0
   1 1 1
----

Vector unit-stride, constant-stride, and indexed (scatter/gather)
load/store instructions are supported.

NOTE: Vector AMO instructions are TBD.

Vector load/store base registers and strides are taken from the GPR
`x` registers.

Vector load/store instructions move bit patterns between vector
register elements and memory.

An illegal instruction exception is raised if the register element is
narrower than the memory operand.

When `vrep` is set to integer, vector load instructions can optionally
sign- or zero-extend narrower memory values into wider vector register
element destinations.

When `vrep` is set to floating-point, then loads will NaN-box narrower
memory values into a wider register element, regardless of signed or
unsigned opcode.

When the m[1:0] field is set to scalar, the vector load/store
instructions move a single value between element 0 of the vector
register and memory.

The unit-stride fault-first load instructions are used to vectorize
loops with data-dependent exit conditions (while loops).  These
instructions execute as a regular load except that they will only take
a trap on element 0.  If an element > 0 raises an exception, that
element and all following elements in the destination vector
register are not modified, and the vector length `vl` is reduced to the
number of elements processed without a trap.

[source]
----
strlen example using unit-stride fault-first instruction

# size_t strlen(const char *str)
# a0 holds *str

strlen:
    mv a3, a0             # Save start
loop:
    setvli a1, x0, vint8  # Vector of bytes
    vldbff.v v1, (a3)     # Get bytes
    csrr a1, vl           # Get bytes read
    add a3, a3, a1        # Bump pointer
    vseq.vi v0, v1, 0     # Set v0[i] where v1[i] = 0
    vmfirst a2, v0        # Find first set bit
    bltz a2, loop         # Not found?

    add a0, a0, a1        # Sum start + bump
    add a3, a3, a2        # Add index
    sub a0, a3, a0        # Subtract start address+bump

    ret
----

NOTE: Strided and scatter-gather fault-first instructions are not
provided as they represent a large security hole, allowing software to
check multiple random pages for accessibility without experiencing a
trap. The unit-stride versions only allow probing a region immediately
contiguous to a known region.

=== Vector load instructions assembler code

=== Assembler syntax

Scalar operations are written in assembler with a `.s` after the
destination vector register specifier.  Vector masking is written as
another vector operand, with `.t` or `.f` indicating if operation
occurs when `v0[0]` is `1` or `0` respectively.  If no masking operand
is specified, unmasked vector execution (`m=11`) is assumed.

`vop v1, v2, v3, vm` implies following combinations:

[source,asm]
----
    vop    v1,   v2, v3, v0.f  # enabled where v0[0]=0,     m=00
    vop    v1,   v2, v3, v0.t  # enabled where v0[0]=1,     m=01
    vop.s  v1,   v2, v3        # scalar opertaion,          m=10
    vop    v1,   v2, v3        # unmasked vector operation, m=11
----

==== unit-stride instructions

[source,asm]
----
    # vd destination, rs1 base address, rs2=x0, vm is mask encoding

    # fixed-size element
    vlb.v    vd, offset(rs1), vm # 8b
    vlh.v    vd, offset(rs1), vm # 16b
    vlw.v    vd, offset(rs1), vm # 32b
    vld.v    vd, offset(rs1), vm # 64b
    vle.v    vd, offset(rs1), vm # SEW
    vle2.v   vd, offset(rs1), vm # 2*SEW
    vle4.v   vd, offset(rs1), vm # 4*SEW
    vle8.v   vd, offset(rs1), vm # 8*SEW

    # first fault versions
    vlbff.v    vd, offset(rs1), vm # 8b
    vlhff.v    vd, offset(rs1), vm # 16b
    vlwff.v    vd, offset(rs1), vm # 32b
    vldff.v    vd, offset(rs1), vm # 64b
    vleff.v    vd, offset(rs1), vm # SEW
    vle2ff.v   vd, offset(rs1), vm # 2*SEW
    vle4ff.v   vd, offset(rs1), vm # 4*SEW
    vle8ff.v   vd, offset(rs1), vm # 8*SEW

    # Scalar versions
    vlb.s vd, offset(rs1)      # 8b scalar load into element 0
          ...
----

NOTE: Could encode unit-stride as constant-stride with rs2=x0, but
this would add to decode complexity.

==== constant-stride instructions
[source,asm]
----
    # vd destination, rs1 base address, rs2 byte stride
    vlsb.v    vd, offset(rs1), rs2, vm # 8b
    vlsh.v    vd, offset(rs1), rs2, vm # 16b
    vlsw.v    vd, offset(rs1), rs2, vm # 32b
    vlsd.v    vd, offset(rs1), rs2, vm # 64b
    vlse.v    vd, offset(rs1), rs2, vm  # SEW
    vlse2.v   vd, offset(rs1), rs2, vm  # 2*SEW
    vlse4.v   vd, offset(rs1), rs2, vm  # 4*SEW
    vlse8.v   vd, offset(rs1), rs2, vm  # 8*SEW

    vlse8.s   vd, offset(rs1), rs2, vm  # 8*SEW scalar load
----

The stride is interpreted as an integer representing a byte offset.

==== indexed (scatter-gather) instructions
[source,asm]
----
    # vd destination, rs1 base address, vs2 indices
    vlxb.v    vd, offset(rs1), vs2, vm  # 8b
    vlxh.v    vd, offset(rs1), vs2, vm  # 16b
    vlxw.v    vd, offset(rs1), vs2, vm  # 32b
    vlxd.v    vd, offset(rs1), vs2, vm  # 64b
    vlxe.v    vd, offset(rs1), vs2, vm  # SEW
    vlxe2.v   vd, offset(rs1), vs2, vm  # 2*SEW
    vlxe4.v   vd, offset(rs1), vs2, vm  # 4*SEW
    vlxe8.v   vd, offset(rs1), vs2, vm  # 8*SEW
----

Scatter/gather indices are treated as signed integers representing
byte offsets.  If stem:[SEW < XLEN], then indices are sign-extended to
stem:[XLEN] before adding to the base.  If stem:[SEW > XLEN], the
indices are taken from the least-significant stem:[XLEN] bits.

NOTE: stem:[SEW] has to be wide enough to hold the indices, which
could mandate larger stem:[SEW] than desired.  Ideally want to support
index vectors wider than stem:[SEW], by adding new vector indexed
loads and stores with double-width or greater vector indices.

=== Vector stores

Vector stores move data values as bits taken from the LSBs of the
source element.  If the store datatype is wider than stem:[SEW], then
multiple vector registers are used to supply the data as described
above.

==== unit-stride store instructions
[source,asm]
----
    vsb.v     vs3, offset(rs1), vm  # 8b
    vsh.v     vs3, offset(rs1), vm  # 16b
    vsw.v     vs3, offset(rs1), vm  # 32b
    vsd.v     vs3, offset(rs1), vm  # 64b
    vse.v     vs3, offset(rs1), vm  # SEW
    vse2.v    vs3, offset(rs1), vm  # 2*SEW
    vse4.v    vs3, offset(rs1), vm  # 4*SEW
    vse8.v    vs3, offset(rs1), vm  # 8*SEW

    vsb.s   vs3, offset(rs1)      # Scalar 8b store from element 0
    ...
----

==== constant-stride store instructions
[source,asm]
----
    vssb.v    vs3, offset(rs1), rs2, vm  # 8b
    vssh.v    vs3, offset(rs1), rs2, vm  # 16b
    vssw.v    vs3, offset(rs1), rs2, vm  # 32b
    vssd.v    vs3, offset(rs1), rs2, vm  # 64b
    vsse.v    vs3, offset(rs1), rs2, vm  # SEW
    vsse2.v   vs3, offset(rs1), rs2, vm  # 2*SEW
    vsse4.v   vs3, offset(rs1), rs2, vm  # 4*SEW
    vsse8.v   vs3, offset(rs1), rs2, vm  # 8*SEW
----

==== indexed store (scatter) instructions (ordered by element)
[source,asm]
----
    vsxb.v    vs3, offset(rs1), vs2, vm  # 8b
    vsxh.v    vs3, offset(rs1), vs2, vm  # 16b
    vsxw.v    vs3, offset(rs1), vs2, vm  # 32b
    vsxd.v    vs3, offset(rs1), vs2, vm  # 64b
    vsxe.v    vs3, offset(rs1), vs2, vm  # SEW
    vsxe2.v   vs3, offset(rs1), vs2, vm  # 2*SEW
    vsxe4.v   vs3, offset(rs1), vs2, vm  # 4*SEW
    vsxe8.v   vs3, offset(rs1), vs2, vm  # 8*SEW
----

==== unordered-indexed (scatter-gather) instructions
[source,asm]
----
    vsuxb.v   vs3, offset(rs1), vs2, vm  # 8b
    vsuxh.v   vs3, offset(rs1), vs2, vm  # 16b
    vsuxw.v   vs3, offset(rs1), vs2, vm  # 32b
    vsuxd.v   vs3, offset(rs1), vs2, vm  # 64b
    vsuxe.v   vs3, offset(rs1), vs2, vm  # SEW
    vsuxe2.v  vs3, offset(rs1), vs2, vm  # 2*SEW
    vsuxe4.v  vs3, offset(rs1), vs2, vm  # 4*SEW
    vsuxe8.v  vs3, offset(rs1), vs2, vm  # 8*SEW
----

NOTE: Dropped reverse-ordered scatter for now, can use rgather to
reverse index order.

NOTE: There is redundancy between all the scalar variants of
unit-stride, constant-stride, and scatter-gather vector load/store
instructions.

=== Vector memory model

Vector memory instructions appear to execute in program order on the
local hart.  Vector memory instructions follow RVWMO at the
instruction level, and element operations are ordered within the
instruction as if performed by an element-ordered sequence of
syntactically independent scalar instructions.  Vector indexed-ordered
stores write elements to memory in element order.

== Vector Arithmetic Instructions

The vector arithmetic instructions use a new major opcode (OP-V =
1010111~2~) which neighbors OP-FP, but generally follow the encoding
pattern of the scalar floating-point instructions under the OP-FP
opcode.

=== Vector-Vector and Vector-Scalar Arithmetic Instructions

Most vector arithmetic instructions have both vector-vector (`.vv`),
where both operands are vectors of elements, and vector-scalar
(`.vs`), where the second operand is a scalar taken from element 0 of
the second source vector register.  A few non-commutative operations
(such as reverse subtract) subtract are encoded with special opcodes.

=== Vector-Immediate Arithmetic Instructions

Many vector arithmetic instructions have vector-immediate forms
(`.vi`) where the second scalar argument is a 5-bit immediate encoded
in `rs2` space.  The immediate is sign-extended to the standard
element width, and interpreted according to the `vtype` setting.

----
vadd.vi vd, vrs1, 3
----

=== Widening Vector Arithmetic Instructions

A few vector arithmetic instructions are defined to be __widening__
operations where the destination elements are stem:[2\times SEW] wide
and are stored in an even-odd vector register pair.  The first operand
can be either single or double-width. These are generally written with
a `w` suffix on the opcode.

=== Mask encoding

All vector arithmetic instructions can be masked according to the
m[1:0] field.

[source]
----
mask encoding m[1:0] is held in inst[26:25]

m[1:0]
  00    vector, where v0[0] = 0
  01    vector, where v0[0] = 1
  10    scalar
  11    always true
----

=== Vector Arithmetic Operand encoding

[source]
----
rm[2:0] field is held in inst[14:12]

Encoding of operand pattern rm field for regular vector arithmetic
instructions.

rm2 rm1 rm0

0     0   0      Vector-vector   SEW =   SEW op SEW
0     0   1      Reserved
0     1   0      Vector-vector 2*SEW =   SEW op SEW
0     1   1      Vector-vector 2*SEW = 2*SEW op SEW

1     0   0      Vector-scalar   SEW =   SEW op s_SEW
1     0   1      Vector-imm      SEW =   SEW op simm[4:0]
1     1   0      Vector-scalar 2*SEW =   SEW op s_SEW
1     1   1      Vector-scalar 2*SEW = 2*SEW op s_SEW
----

Bit `rm[2]` selects between vector second source or scalar
second source.

Bit `rm[1]` selects whether the destination is twice the width of
stem:[SEW].

Bit `rm[0]` selects whether the first operand is one or two times the stem:[SEW] or whether the second operand is a 5-bit sign-extended immediate held in the `rs2` field.

The 5-bit immediate field is always treated as a signed integer and
sign-extended to stem:[SEW] bits, regardless of `vtype` setting.

NOTE: For floating-point representation, the 5-bit immediate can be
used to supply 0.0.

[source]
----
Assembly syntax pattern for vector arithmetic instructions

vop.vv  vd, vs1, vs2, vm    # vector-vector operation
vop.vs  vd, vs1, rs2, vm    # vector-scalar operation
vop.vi  vd, vs1, imm, vm    # vector-immediate operation

vopw.vv  vd, vs1, vs2, vm    # 2*SEW = SEW op SEW
vopw.vs  vd, vs1, rs2, vm    # 2*SEW = SEW op SEW

vopw.wv  vd, vs1, vs2, vm    # 2*SEW= 2*SEW op SEW
vopw.ws  vd, vs1, rs2, vm    # 2*SEW= 2*SEW op SEW
----

=== Legal Vector Arithmetic Instructions

The following vector arithmetic instructions are provided

[source]
----
         .vv .vs .vi w.vv w.vs w.wv w.ws
VADD      x   x   x   x    x    x    x
VSUB      x   x   x   x    x    x    x

VAND      x   x   x
VOR       x   x   x
VXOR      x   x   x

VSLL      x   x   x
VSRL      x   x   x
VSRA      x   x   x
VCLIP     x   x   x
VCLIPU    x   x   x

VSEQ      x   x   x
VSNE      x   x   x
VSLT      x   x   x
VSLTU     x   x   x
VSLE      x   x   x
VSLEU     x   x   x

VMUL      x   x   x   x    x    x    x
VMULU     x   x   x   x    x    x    x
VMULSU    x   x   x   x    x    x    x
VMULH     x   x   x

VDIV      x   x   x
VDIVU     x   x   x
VREM      x   x   x
VREMU     x   x   x

VSQRT     x   x   x

VFSGNJ    x   x   x
VFSGNJN   x   x   x
VFSGNJX   x   x   x

VMIN      x   x   x
VMAX      x   x   x

VFCLASS   x   x   x

VCVT
----

==== Explicit FP instructions?

NOTE: There's a discussion around whether to add explicit
floating-point versions of standard arithmetic instructions.  In
addition to above, which includes some that are already explicitly
floating-point, would require the following are added to base.

[source]
----
         .vv .vs .vi w.vv w.vs w.wv w.ws
VFADD      x   x      x    x    x    x
VFSUB      x   x      x    x    x    x

VFSEQ      x   x
VFSNE      x   x
VFSLT      x   x
VFSLE      x   x
(possibly unordered variants also)

VFMUL      x   x       x    x    x    x

VFDIV      x   x
----

NOTE: It is not possible to provide both forms of fused multiply-add
instructions.  On option is to dedicate the existing opcodes to
floating-point and add a second set of destructive fused
mul-accumulate for integer.

=== Vector Comparison Instructions

The following compare instructions write `1` to the destination
register if the comparison evaluates to true and produces `0`
otherwise.

[NOTE] `VSNE` is not needed with complementing masks but sometimes
predicate results feed into things other than predicate inputs and so
`VSNE` can save an instruction.

[NOTE]: Need to revisit vector floating-point unordered compare
instructions.

[source,asm]
----
    vseq.vv    vd, vs1, vs2, vm
    vseq.vs    vd, vs1, rs2, vm
    vseq.vi    vd, vs1, imm, vm

    vsne.vv    vd, vs1, vs2, vm
    vsne.vs    vd, vs1, rs2, vm
    vsne.vi    vd, vs1, imm, vm

    ...
----

These conditionals effectively `AND` in the mask when producing
`0`/`1` in output, e.g,

[source,asm]
----
    # (a < b) && (b < c) in two instructions
    vslt.vv    v0, va, vb
    vslt.vv    v0, vb, vc, vm
----

The combination of VLT and VLTE can cover all cases, including
compares with scalars by complementing results:

[source]
----
v = s ,  ! (v = s) = (v != s)
v < s ,  ! (v < s) = (v >= s)
v <= s , ! (v <=s) = (v > s)
----

=== Vector Merge Instruction

The vector merge instruction combines two source operands based on the
mask field.

[source]
----
vmerge.vv vd, vs1, vs2, vm  # vd[i] = vm[i] ? vs2[i] : vs1[i]
vmerge.vs vd, vs1, vs2, vm  # vd[i] = vm[i] ? vs2[0] : vs1[i]
vmerge.vi vd, vs1, imm, vm  # vd[i] = vm[i] ? imm    : vs1[i]
----

The second operand is written where the mask is true.

NOTE: The `vmerge.vi` instruction can be used to initialize a vector
register with an immediate value, and the `vmerge.vs` instruction can
be used to __splat__ a scalar value into all elements of a vector.

=== Vector multiply/divide

These are all equivalent to scalar integer multiply/divides, and
operate on VSEW source and destination widths.

[source,asm]
----
    vmul.vv      vd, vs1, vs2, vm
    vmulh.vv     vd, vs1, vs2, vm
    vmulhsu.vv   vd, vs1, vs2, vm
    vmulhu.vv    vd, vs1, vs2, vm
    vdiv.vv      vd, vs1, vs2, vm
    vdivu.vv     vd, vs1, vs2, vm
    vrem.vv      vd, vs1, vs2, vm
    vremu.vv     vd, vs1, vs2, vm

Also have .vs and .vi variants
----

== Vector Narrowing instructions

A few instructions are provided to convert multi-width vectors into
single-width vectors.

[source]
----
 VSRLN  vector shift right logical narrowing
 VSRAN  vector shift right arithmetic narrowing
 VCLIPN   vector clip after shift right narrowing
 VCLIPUN  vector clip unsigned after shift right narrowing

 vd[i] = clip(round(vs1[i] + rnd) >> vs2[i])
----

For VSRLN/VSRAN, clip=nop, rnd = nop.  These operations can be used to
extract a field from a wider structure held in a wider element.

For VCLIPN, the value is treated as a signed integer and saturates if
result would overflow the destination.

For VCLIPUN, the value is treated as a signed integer and saturates if
result would overflow the destination.

For VCLIPN/VCLIPUN, the rounding mode is specified in the `fcsr` in a
new `vxrm[1:0]` field.  Rounding occurs around the LSB of the
destination.

There are also regular non-narrowing VCLIP instructions defined with
same function.

[source]
----
 `vxrm[1:0]`
 Holds fixed-point rounding mode.

 00      rup   round-up (+0.5 LSB)
 01      rne   round to nearest-even
 10      trn   truncate
 11      jam   jam (OR bits into LSB)
----

The narrowing instructions used a different operand encoding in
`rm[2:0]`.

[source]
----
# vs1 = 2*SEW, 4*SEW

 rm2 rm1 rm0

 0     0   0      Vector-vector  SEW =  2*SEW op SEW
 0     0   1      Reserved
 0     1   0      Vector-vector  SEW =  4*SEW op SEW
 0     1   1      Reserved

 1     0   0      Vector-scalar  SEW =  2*SEW op SEW
 1     0   1      Vector-imm     SEW =  2*SEW op imm
 1     1   0      Vector-scalar  SEW =  4*SEW op SEW
 1     1   1      Vector-imm     SEW =  4*SEW op imm
----

[source]
----
vclipn.vv vd, vs1, vs2, vm  # SEW = 2*SEW >> SEW
vclipn.vs vd, vs1, rs2, vm  # SEW = 2*SEW >> SEW
vclipn.vi vd, vs1, imm, vm  # SEW = 2*SEW >> imm

vclipn.wv vd, vs1, vs2, vm  # SEW = 4*SEW >> SEW
vclipn.ws vd, vs1, rs2, vm  # SEW = 4*SEW >> SEW
vclipn.wi vd, vs1, imm, vm  # SEW = 4*SEW >> imm

vclipun.vv vd, vs1, vs2, vm  # SEW = 2*SEW >> SEW
vclipun.vs vd, vs1, rs2, vm  # SEW = 2*SEW >> SEW
vclipun.vi vd, vs1, imm, vm  # SEW = 2*SEW >> imm

vclipun.wv vd, vs1, vs2, vm  # SEW = 4*SEW >> SEW
vclipun.ws vd, vs1, rs2, vm  # SEW = 4*SEW >> SEW
vclipun.wi vd, vs1, imm, vm  # SEW = 4*SEW >> imm

vsrln.vv vd, vs1, vs2, vm  # SEW = 2*SEW >> SEW
vsrln.vs vd, vs1, rs2, vm  # SEW = 2*SEW >> SEW
vsrln.vi vd, vs1, imm, vm  # SEW = 2*SEW >> imm

vsrln.wv vd, vs1, vs2, vm  # SEW = 4*SEW >> SEW
vsrln.ws vd, vs1, rs2, vm  # SEW = 4*SEW >> SEW
vsrln.wi vd, vs1, imm, vm  # SEW = 4*SEW >> imm

vsran.vv vd, vs1, vs2, vm  # SEW = 2*SEW >> SEW
vsran.vs vd, vs1, rs2, vm  # SEW = 2*SEW >> SEW
vsran.vi vd, vs1, imm, vm  # SEW = 2*SEW >> imm

vsran.wv vd, vs1, vs2, vm  # SEW = 4*SEW >> SEW
vsran.ws vd, vs1, rs2, vm  # SEW = 4*SEW >> SEW
vsran.wi vd, vs1, imm, vm  # SEW = 4*SEW >> imm
----


== Vector fused Multiply-Adds

The standard scalar floating-point fused multiply-adds occupy four
major opcodes.

There are two unused rounding modes that can be used to encode vector
fused multiply-adds, in both vector-vector and vector-scalar forms,
where the scalar is one input to the multiply.  When a scalar input to
the add is needed, this can be provided by splatting the value to a
vector.

[source]
----
rm2 rm1 rm0
 1   0   1      Vector-vector  vd = vs3 + vs1 * vs2
 1   1   0      Vector-scalar  vd = vs3 + vs1 * rs2
----

The FNMADD and FNMSUB variants are dropped in favor of widening vector
operations, which treat the add input and final result as
double-width.

[source]
----
VMADD     SEW = SEW + SEW*SEW
VMSUB     SEW = SEW + SEW*SEW
VMADDW  2*SEW = 2*SEW + SEW*SEW
VMSUBW  2*SEW = 2*SEW + SEW*SEW
----

=== Vector fused-multiply-add instructions

[source]
----
  vmadd.vvv vd, vs1, vs2, vs3, vm
  vmadd.vvs vd, vs1, rs2, vs3, vm
  vmaddw.vvv vd, vs1, vs2, vs3, vm
  vmaddw.vvs vd, vs1, rs2, vs3, vm
  vmsub.vvv vd, vs1, vs2, vs3, vm
  vmsub.vvs vd, vs1, rs2, vs3, vm
  vmsubw.vvv vd, vs1, vs2, vs3, vm
  vmsubw.vvs vd, vs1, rs2, vs3, vm
----

Additional fused multiply-add operations can be provided as
destructive operations in the regular vector arithmetic encoding
space.

=== Vector Reduction Operations

These instructions take a vector and scalar (vs2[0]) as input, and
produces a scalar result (vd[0]) that is a reduction over the source
scalar and vector.  Masked elements are ignored in the reduction.

[source,asm]
----
    vredsum.v   vd, vs1, vs2, vm #   SEW = SEW   + sum(SEW)
    vredsumw.v  vd, vs1, vs2, vm # 2*SEW = 2*SEW + sum(SEW)
    vredmax.v   vd, vs1, vs2, vm
    vredmaxu.v  vd, vs1, vs2, vm
    vredmin.v   vd, vs1, vs2, vm
    vredminu.v  vd, vs1, vs2, vm
    vredand.v   vd, vs1, vs2, vm
    vredor.v    vd, vs1, vs2, vm
    vredxor.v   vd, vs1, vs2, vm
----

By default, when the operation is non-associative (e.g.,
floating-point addition) the reductions are specified to occur as if
done in sequential element order, but a user `fcsr` mode bit can
specify that unordered reductions are allowed.  In this case, the
reduction result must match some ordering of the individual sequential
operations.

A widening form of the sum reduction is provided that writes a
double-width reduction result.

== Vector Mask Operations

Several operations are provided to help operate on mask bits held in
the LSB of elements of a vector register.

=== `vmpopc` mask population count

[source]
----
    vmpopc rd, vs1, vm
----

The `vmpopc` instruction counts the number of elements of the first
`vl` elements of the vector source that have their low bit set,
excluding elements where the mask is false, and writes the result to a
GPR.

=== `vmfirst` find-first-set mask bit

[source]
----
    vmfirst rd, vs1, vm
----

The `vmfirst` instruction finds the lowest-numbered element of the
source vector that has its LSB set excluding elements where the mask
is false, and writes that element's index to a GPR.  If no element has
an LSB set, it writes -1 to the GPR.

=== Vector Iota instruction

The VIOTA instruction reads `v0` and writes to each element of the
destination the sum of all the least-significant bits of elements in
the mask selected by m[1:0] with index less than the element, e.g., a
parallel prefix sum of the mask values.

If the value would overflow the destination, the least-significant
bits are retained.  This instruction is not masked, so writes all `vl`
elements of destination vector.

[source]
----
 viota.v vd        # Unmasked, writes index to each element, vd[i] = i
 viota.v vd, v0.t  # Writes to each element, sum of preceding true elements.

 # Example

     7 6 5 4 3 2 1 0   Element number
     1 0 0 1 0 0 0 1   v0 contents

     7 6 5 4 3 2 1 0   viota.v vd
     2 2 2 1 1 1 1 0   viota.v vd, v0.t
     5 4 3 3 2 1 0 0   viota.v vd, v0.f
----

NOTE: The `viota` instruction can be combined with scatter/gather
instructions to perform vector compress/expand instructions.

NOTE: Could take any argument register not just implicit mask v0.

=== `vmsbf.v` set-before-first mask bit

[source]
----
    vmsbf.v vd, vs1, vm

 # Example

     7 6 5 4 3 2 1 0   Element number

     1 0 0 1 0 1 0 0   v3 contents
                       vmsbf.v v2, v3
     0 0 0 0 0 0 1 1   v2 contents

     1 0 0 1 0 1 0 1   v3 contents
                       vmsbf.v v2, v3
     0 0 0 0 0 0 0 0   v2

     0 0 0 0 0 0 0 0   v3 contents
                       vmsbf.v v2, v3
     1 1 1 1 1 1 1 1   v2

     1 1 0 0 0 0 1 1   v0 vcontents
     1 0 0 1 0 1 0 0   v3 contents
                       vmsbf.v v2, v3, v0.t
     0 1 x x x x 1 1   v2 contents
----

The `vmsbf.v` instruction writes a 1 to all active elements before the
first source element that has a set LSB, then writes a zero to that
element and all following active elements.  If there is no set bit in
the source vector, the all active elements in the destination are
written with a 1.

=== `vmsif.v` set-including-first mask bit

[source]
----
    vmsif.v vd, vs1, vm

 # Example

     7 6 5 4 3 2 1 0   Element number

     1 0 0 1 0 1 0 0   v3 contents
                       vmsif.v v2, v3
     0 0 0 0 0 1 1 1   v2 contents

     1 0 0 1 0 1 0 1   v3 contents
                       vmsif.v v2, v3
     0 0 0 0 0 0 0 1   v2

     1 1 0 0 0 0 1 1   v0 vcontents
     1 0 0 1 0 1 0 0   v3 contents
                       vmsif.v v2, v3, v0.t
     1 1 x x x x 1 1   v2 contents
----

The vector mask set-including-first instruction is similar to
set-before-first, except it also includes the element with a set bit.

=== `vmsof.v` set-only-first mask bit

[source]
----
    vmsof.v vd, vs1, vm

 # Example

     7 6 5 4 3 2 1 0   Element number

     1 0 0 1 0 1 0 0   v3 contents
                       vmsof.v v2, v3
     0 0 0 0 0 1 0 0   v2 contents

     1 0 0 1 0 1 0 1   v3 contents
                       vmsof.v v2, v3
     0 0 0 0 0 0 0 1   v2

     1 1 0 0 0 0 1 1   v0 vcontents
     1 1 0 1 0 1 0 0   v3 contents
                       vmsof.v v2, v3, v0.t
     0 1 x x x x 0 0   v2 contents
----

The vector mask set-including-first instruction is similar to
set-before-first, except it only sets the first element with a bit
set, if any.

=== Example using vector mask instructions

The following is an example of vectorizing a data-dependent exit loop.

[source]
----
  # char* strcpy(char *dst, const char* src)
strcpy:
      mv a2, a0               # Copy dst
loop:
    setvli x0, x0, vint8    # Max length vectors of bytes
    vlbff.v v1, (a1)        # Get src bytes
      csrr t1, vl           # Get number of bytes fetched
    vseq.vi v0, v1, 0       # Flag zero bytes
    vmfirst a3, v0          # Zero found?
      add a1, a1, t1        # Bump pointer
    vmsif.v v0, v0          # Set mask up to and including zero byte.
    vsb.v v1, (a2), v0.t    # Write out bytes
      add a2, a2, t1        # Bump pointer
      bltz a3, loop

      ret

  # char* strncpy(char *dst, const char* src, size_t n)
strncpy:
      mv a3, a0               # Copy dst
loop:
    setvli x0, a2, vint8    # Vectors of bytes.
    vlbff.v v1, (a1)        # Get src bytes
    vseq.vi v0, v1, 0       # Flag zero bytes
    vmfirst a4, v0          # Zero found?
    vmsif.v v0, v0          # Set mask up to and including zero byte.
    vsb.v v1, (a3), v0.t    # Write out bytes
      bgez a4, exit         # Done
      csrr t1, vl           # Get number of bytes fetched
      add a1, a1, t1        # Bump pointer
      sub a2, a2, t1        # Decrement count.
      add a3, a3, t1        # Bump pointer
      bnez a2, loop         # Anymore?

exit:
      ret

----


== Vector Permutation Instructions

A range of permutation instructions are provided.

=== Insert/Extract

The first form of insert/extract operations transfer a single value
between a GPR and one element of a vector register.  A second scalar
GPR operand gives the element index, treated as an unsigned integer.
If the index is out of range on a vector extract, then zero is
returned for the element value.  If the index is out of range (i.e.,
stem:[>VLMAX]) for a vector insert, the write is ignored.

[source]
----
vmv.x.v rd, vs1, rs2  # rd = vs1[rs2]
vmv.v.x vd, rs1, rs2  # vd[rs2] = rs1
----

The second form of insert/extract transfers a single value between
element 0 of one vector register and one indexed element of a second
vector register.

[source]
----
vmv.s.v vd, vs1, rs2 # vd[0] = vs1[rs2]
vmv.v.s vd, vs1, rs2 # vd[rs2] = vs1[0]
----

=== Slides

The slide instructions move elements up and down a vector.

[source]
----
 vslideup.vs vd, vs1, rs2, vm   # vd[i+rs2] = vs1[i]
 vslideup.vi vd, vs1, imm, vm   # vd[i+imm] = vs1[i]
----

For `vslideup`, the value in `vl` specifies the number of source
elements that are read.  The destination elements below the start
index are left undisturbed.  Destination elements past `vl` can be
written, but writes past the end of the destination vector are
ignored.

[source]
----
 vslidedown.vs vd, vs1, rs2, vm # vd[i] = vs1[i+rs2]
 vslidedown.vi vd, vs1, imm, vm # vd[i] = vs1[i+imm]
----

For `vslidedown`, the value in `vl` specifies the number of
destination elements that are written.  Elements in the source vector
can be read past `vl`.  If a source vector index is out of range, zero
is returned for the element.

=== Register Gather

This instruction reads elements from a source vector at locations
given by a second source element index vector.  The values in the
index vector are treated as unsigned integers. The number of elements
to write to the destination register is given by `vl`.  The source
vector can be read at any index, stem:[index < VLMAX ].

[source]
----
vrgather.vv vd, vs1, vs2, vm # vd[i] = vs1[vs2[i]]
----

If the element indices are out of range ( stem:[ vs2[i\] \geq VLMAX] )
then zero is returned for the element value.

== Exception Handling

Different platforms require different treatment of exceptions.

=== `*progress` CSR

In all platforms, a new "progress" CSR is defined and is made visible
in any privilege mode that can observe a partially completed vector
instruction.

On a trap during a vector instruction, the existing `*epc` CSR is
written with a pointer to the errant vector instruction, while the
`*progress` CSR contains the element index that caused the trap to be
taken.

All vector instructions are defined to begin execution with the
element number given in the `*progress` CSR, leaving earlier elements
in the vector undisturbed, and to reset the `*progress` CSR to zero
for the start of the subsequent vector instruction.

If the value in the `*progress` register is >= `vl` then no elemental
operations are performed, and the `*progress` register is reset to
zero.

=== Precise vector traps

Precise vector traps, require that:

 - all instructions older than the trapping vector instruction have committed their results
 - no instructions newer than the trapping vector instruction have altered architectural state
 - any operations within the trapping vector instruction affecting result elements preceding the index in the `*progress` CSR have committed their results
 - no operations within the trapping vector instruction affecting elements at or following the `*progress` CSR have committed their results

NOTE: We assume most supervisor-mode environments will require precise
vector traps.

NOTE: We chose to add a progress CSR to allow resumption of a
partially executed vector instruction.  This matches the scheme in the
IBM 3090 vector facility.  Without the progress CSR, to ensure forward
progress implementations would have to guarantee an entire vector
instruction can always complete atomically without generating a trap.
This is particularly difficult to ensure in the presence of
constant-stride or scatter/gather operations.

=== Imprecise vector traps

Imprecise vector traps are traps that are not precise.  In particular,
instructions newer than `*epc` may have committed results, and
instructions older than `*epc` may have not completed execution.
Imprecise traps are primarily intended to be used in situations where
reporting an error and terminating execution is the appropriate
response.

NOTE: A platform might specify that interrupts are precise while other
traps are imprecise.  We assume many embedded platforms will only
generate imprecise traps for vector instructions on fatal errors, so
do not require resumable traps.

=== Selectable precise/imprecise traps

Some platforms may choose to provide a mode bit in a CSR to select
between precise and imprecise vector traps.  Precise mode would run
more slowly, but support debugging of errors, while imprecise mode
would run at high-performance but possibly obscure error conditions.

NOTE: It is acknowledged that some errors will only manifest in
imprecise mode.

=== Swappable traps

Another trap mode can support swappable state in the vector unit,
where on a trap, special instructions can save and restore the vector
unit microarchitectural state, to allow execution to continue
correctly around imprecise traps.

This is not defined in base vector ISA.

== Vector Assembly Code Examples

The following are provided as non-normative text to help explain the vector ISA.

=== Vector-vector add example

[source]
----
    # vector-vector add routine of 32-bit integers
    # void vvaddint32(size_t n, const int*x, const int*y, int*z)
    # { for (size_t i=0; i<n; i++) { z[i]=x[i]+y[i]; } }
    #
    # a0 = n, a1 = x, a2 = y, a3 = z
    # Non-vector instructions are indented
vvaddint32:
    vsetvli t0, a0, vint32 # Set vector length based on 32-bit vectors
    vlw.v v0, (a1)           # Get first vector
      sub a0, a0, t0         # Decrement number done
      slli t0, t0, 2         # Multiply number done by 4 bytes
      add a1, a1, t0         # Bump pointer
    vlw.v v1, (a2)           # Get second vector
      add a2, a2, t0         # Bump pointer
    vadd.v v2, v0, v1        # Sum vectors
    vsw.v v2, (a3)           # Store result
      add a3, a3, t0         # Bump pointer
      bnez a0, vvaddint32    # Loop back
      ret                    # Finished
----

=== Memcpy example

[source]
----
    # void *memcpy(void* dest, const void* src, size_t n)
    # a0=dest, a1=src, a2=n
    #
  memcpy:
      mv a3, a0 # Copy destination
  loop:
    vsetvli t0, a2, vint8,vlmul8  # Vectors of 8b
    vlb.v v0, (a1)                # Load bytes
      add a1, a1, t0              # Bump pointer
      sub a2, a2, t0              # Decrement count
    vsb.v v0, (a3)                # Store bytes
      add a3, a3, t0              # Bump pointer
      bnez a2, loop               # Any more?
      ret                         # Return
----

=== Conditional example

[source]
----
       (int16) z[i] = ((int8) x[i] < 5) ? (int16) a[i] : (int16) b[i];

Fixed 16b SEW:
loop:
    vsetvli t0, a0, vint16  # Use 16b elements.
    vlb.v v0, (a1)               # Get x[i], sign-extended to 16b
      sub a0, a0, t0           # Decrement element count
      add a1, a1, t0           # x[i] Bump pointer
    vslti v0, v0, 5            # Set mask in v0
      slli t0, t0, 1             # Multiply by 2 bytes
    vlh.v v1, (a2), v0.t         # z[i] = a[i] case
      add a2, a2, t0           # a[i] bump pointer
    vlh.v v1, (a3), v0.f         # z[i] = b[i] case
      add a3, a3, t0           # b[i] bump pointer
    vsh.v v1, (a4)               # Store z
      add a4, a4, t0           # b[i] bump pointer
      bnez a0, loop
----

=== VLmul example

Out of date.

[source]
----
    # Example using vlmul
    # Same as before, except now VLEN is 8 times greater.
strlen:
    mv a3, a0             # Save start
loop:
    setvli a1, x0, vint8, vreg4  # Vector of bytes, only use 4 vregs
    vldbff.v v1, (a3)     # Get bytes
    csrr a1, vl           # Get bytes read
    add a3, a3, a1        # Bump pointer
    vseq.vi v0, v1, 0     # Set v0[i] where v1[i] = 0
    vmfirst a2, v0        # Find first set bit
    bltz a2, loop         # Not found?

    add a0, a0, a1        # Sum start + bump
    add a3, a3, a2        # Add index
    sub a0, a3, a0        # Subtract start address+bump

    ret

----



== Expanded SEW encoding

As an extension, the vsew field is extended with three upper bits.

[source]
----
  vsew[2:0] (standard element width) encoding

  vsew[2:0]   SEW
  ---        ----
  000           8
  001          16
  010          32
  011          64
  100         128
  101         256
  110         512
  111        1024

  vxsew[5:0] (expanded element width) encoding

  vxsew[5:0]  SEW
  ---        ----
  000000       8
  001000       1
    ...          1..8, steps of 1
  111000       7
  000001      16
  001001       9
    ...          9..16, steps of 1
  111001      15
  000010      32
  001010      18
    ...          18-32, steps of 2
  111010      30

  ...TBD

----

== Three-operand addition instructions

These instructions are not considered part of the base.

To support multi-word arithmetic, three input add instructions are
defined:

[source]
----
 vadd3      vd, vs1, vs2, vs3, vm   # 3-input sum
 vadd3cout  vd, vs1, vs2, vs3, vm   # 3-input carry-out
----

The `vadd3` instruction sums three integer values together.

The `vadd3cout` instruction produces the carry-out from summing the
three integer values together.


== Notes

[source]
----

 ELEN=64b

 Byte        7 6 5 4 3 2 1 0

 SEW=8b      7 6 5 4 3 2 1 0
 SEW=16b       3   2   1   0
 SEW=32b           1       0
 SEW=64b                   0

 ELEN=128b

 Byte        F E D C B A 9 8 7 6 5 4 3 2 1 0

 SEW=8b      F E D C B A 9 8 7 6 5 4 3 2 1 0
 SEW=16b       7   6   5   4   3   2   1   0
 SEW=32b           3       2       1       0
 SEW=64b                   1               0
 SEW=128b                                  0

 ELEN=128b

 Byte        F E D C B A 9 8 7 6 5 4 3 2 1 0

 SEW=16b       7   6   5   4   3   2   1   0  v0
 SEW=32b           6       4       2       0  v2
 SEW=32b           7       5       3       1  v3


 ELEN=128b

 Byte        F E D C B A 9 8 7 6 5 4 3 2 1 0

 SEW=8b      F E D C B A 9 8 7 6 5 4 3 2 1 0
 SEW=16b       7   6   5   4   3   2   1   0
 SEW=32b           3       2       1       0
 SEW=64b                   1               0
 SEW=128b                                  0
----

== Alternative design with floating-point registers overlaid on vector registers

NOTE: This text is retained from earlier 0.6 design, which had "F in
V", but which is not now the plan of record.



If the system has floating-point registers, the floating-point
register ``f``__x__ is contained in the FLEN lowest-numbered bits of
vector register ``v``__x__.

[source]
----
Example, FLEN=32, VLEN=64

bytes        7 6 5 4 3 2 1 0
            |       v0      |
                    |  f0   |
----

NOTE: There has been discussion of a desire to keep f registers
separated, with an option described below.

=== Interaction of vectors and standard scalar floating-point code

The vector extension does not modify the behavior of standard scalar
floating-point instructions.  Standard scalar floating-point
instructions operate on the lower FLEN bits of each vector
register, and perform NaN-boxing on floating-point results that
are narrower than FLEN.

NOTE: Requiring all VLEN bits to be NaN-boxed would simplify
microarchitectures that have separate FP and vector physical register
files.  Alternatively, the high VLEN-FLEN bits could be zeroed.

NOTE: The standard scalar floating-point loads and stores move
uninterpreted bit patterns between memory and registers and can be
used to load and store the lower bits of a vector register, using a
wider immediate offset than the vector extension scalar load and store
instructions.  However, microarchitectures using floating-point
recoding techniques or with separate FP and vector physical register
files might experience a performance penalty when using scalar
floating-point loads and stores to move values used as
non-floating-point values.

=== Interaction with standard FP calling conventions

The standard Unix calling convention for floating-point registers is shown below:

[source]
----
Register  ABI Name    Description                  Saver  Number
f07      ft07       FP temporaries               Caller    8
f89      fs01       FP saved registers           Callee    2
f1011    fa01       FP arguments/return values   Caller    2
f1217    fa27       FP arguments                 Caller    6
f1827    fs211      FP saved registers           Callee   10
f2831    ft811      FP temporaries               Caller    4
----

The standard Unix calling convention requires 12 FP registers
(`fs0`-`fs11`) to be saved and restored by the callee to use all
registers for vector code, leaving 20 FP registers that can be used
directly by a vector routine.  However, these 20 FP registers are
spread across the vector register numbers, impeding use of aligned
groups of vector registers.

The following table summarizes the number of vector registers
available at entry to a Unix-ABI function without saving and restoring
scalar floating-point values, including argument registers.

[source]
----
vlmul   Available   Registers
00          20      v0-7,v10-17,v28-31
01          10      v0,v2,v4,v6,v10,v12,v14,v16,v28,v30
10           4      v0,v4,v12,v28
11           1      v0
----

NOTE: Some scalar floating-point argument values will only be
available at some `vlmul` settings.

NOTE: Multiple scalar floating-point register values could be
saved/restored with one vector memory instruction using the `vlmul`
field in `vltype`, but because the callee-saved floating-point
registers are scattered in the number space it is more effective to
use scalar floating-point loads and stores, which have a large offset
field and can also be compressed.

NOTE: The proposed embedded calling convention passes floating-point
arguments in integer registers and treats all floating-point and
vector state as caller-save, so all vector registers would be
available on function entry, but floating-point arguments would need
to be copied to scalars.

